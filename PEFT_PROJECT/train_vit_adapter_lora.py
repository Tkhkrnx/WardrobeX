import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import ViTModel, ViTImageProcessor
from peft import (
    get_peft_model,
    LoraConfig,
)
from data.fashion_dataset import FashionDataset
import numpy as np
from torchvision import transforms
from PIL import Image


class ContrastiveLoss(nn.Module):
    """å¯¹æ¯”æŸå¤±å‡½æ•°"""

    def __init__(self, temperature=0.5):
        super(ContrastiveLoss, self).__init__()
        self.temperature = temperature

    def forward(self, features):
        """
        è®¡ç®—å¯¹æ¯”æŸå¤± - è‡ªç›‘ç£æ–¹å¼
        features: [batch_size, feature_dim]
        """
        # æ ‡å‡†åŒ–ç‰¹å¾
        features = F.normalize(features, dim=1)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.matmul(features, features.T) / self.temperature

        # åˆ›å»ºæ­£æ ·æœ¬å¯¹çš„æ©ç ï¼ˆå‡è®¾ç›¸é‚»æ ·æœ¬æ˜¯æ­£æ ·æœ¬å¯¹ï¼‰
        batch_size = features.shape[0]
        pos_mask = torch.zeros_like(similarity_matrix)

        # å¯¹äºè‡ªç›‘ç£å­¦ä¹ ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ‰¹æ¬¡ä¸­çš„æ ·æœ¬ä¸å…¶å¢å¼ºç‰ˆæœ¬é…å¯¹
        # è¿™é‡Œå‡è®¾å‰åŠéƒ¨åˆ†å’ŒååŠéƒ¨åˆ†æ˜¯é…å¯¹çš„å¢å¼ºæ ·æœ¬
        for i in range(batch_size // 2):
            pos_mask[i, i + batch_size // 2] = 1
            pos_mask[i + batch_size // 2, i] = 1

        # è®¡ç®—å¯¹æ¯”æŸå¤±
        exp_sim = torch.exp(similarity_matrix)

        # æ­£æ ·æœ¬å¯¹
        pos_pairs = exp_sim * pos_mask
        # æ‰€æœ‰æ ·æœ¬å¯¹ï¼ˆé™¤äº†è‡ªå·±ï¼‰
        all_pairs = exp_sim * (1 - torch.eye(batch_size).to(features.device))

        # InfoNCEæŸå¤±
        loss = -torch.log((pos_pairs.sum(1) + 1e-8) / (all_pairs.sum(1) + 1e-8))
        return loss.mean()


class FashionDatasetWithAugmentation(FashionDataset):
    """å¸¦æ•°æ®å¢å¼ºçš„æ—¶å°šæ•°æ®é›†"""

    def __init__(self, extractor=None, root='outputs/train/fashiongen_crops', augment=True):
        super().__init__(extractor=extractor, root=root)
        self.augment = augment
        if augment:
            self.augmentation = transforms.Compose([
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomRotation(degrees=10),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
                transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),
            ])
        else:
            self.augmentation = None

    def __getitem__(self, idx):
        item = super().__getitem__(idx)

        if self.augment and self.augmentation:
            # è·å–åŸå§‹å›¾åƒ
            image_path = self.image_paths[idx]
            original_image = Image.open(image_path).convert("RGB")

            # åº”ç”¨æ•°æ®å¢å¼º
            augmented_image = self.augmentation(original_image)

            # å¤„ç†å¢å¼ºå›¾åƒ
            if self.extractor:
                augmented_pixel_values = self.extractor(images=augmented_image, return_tensors="pt")[
                    "pixel_values"].squeeze(0)
            else:
                augmented_pixel_values = self.default_transform(augmented_image)

            # è¿”å›åŸå§‹å’Œå¢å¼ºçš„å›¾åƒ
            item['augmented_pixel_values'] = augmented_pixel_values

        return item


def train_vit_lora_optimized():
    # === åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œå›¾åƒå¤„ç†å™¨ ===
    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')

    # ä½¿ç”¨æ··åˆç²¾åº¦åŠ è½½æ¨¡å‹ï¼Œä½†ä¸ç›´æ¥è½¬æ¢ä¸ºhalf()
    base_model = ViTModel.from_pretrained(
        'google/vit-base-patch16-224',
        torch_dtype=torch.float16  # ä½¿ç”¨åŠç²¾åº¦åŠ è½½æƒé‡
    )

    # === LoRA é…ç½® ===
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        bias="none",
        target_modules=["query", "value"],
        inference_mode=False,
        init_lora_weights=True
    )
    model = get_peft_model(base_model, lora_config)
    model = model.cuda()  # åªç§»åˆ°GPUï¼Œä¸è½¬æ¢ä¸ºhalf()

    # === åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨å¸¦å¢å¼ºçš„æ•°æ®é›†ï¼‰===
    train_ds = FashionDatasetWithAugmentation(extractor=processor, root='outputs/train/fashiongen_crops', augment=True)
    val_ds = FashionDatasetWithAugmentation(extractor=processor, root='outputs/val/fashiongen_crops',
                                            augment=False)  # éªŒè¯é›†ä¸å¢å¼º

    # ä¼˜åŒ–æ•°æ®åŠ è½½ï¼ˆå……åˆ†åˆ©ç”¨24Gæ˜¾å­˜ï¼‰
    train_loader = DataLoader(
        train_ds,
        batch_size=64,  # å¢å¤§æ‰¹é‡å¤§å°
        shuffle=True,
        num_workers=8,  # å¢åŠ æ•°æ®åŠ è½½è¿›ç¨‹
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=4
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=64,  # éªŒè¯é›†ä¹Ÿå¢å¤§æ‰¹é‡
        num_workers=4,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=4
    )

    # ä¼˜åŒ–ä¼˜åŒ–å™¨è®¾ç½®
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

    # ä½¿ç”¨å¯¹æ¯”æŸå¤±
    contrastive_loss_fn = ContrastiveLoss(temperature=0.5)

    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler()

    os.makedirs("outputs", exist_ok=True)

    # === è®­ç»ƒé…ç½® ===
    num_epochs = 15  # å‡å°‘è½®æ¬¡ä»¥åŠ å¿«è®­ç»ƒ
    best_val_loss = float('inf')
    patience = 8
    patience_counter = 0

    train_losses = []
    val_losses = []

    print(f"å¼€å§‹è®­ç»ƒï¼š{len(train_ds)} ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œ{len(val_ds)} ä¸ªéªŒè¯æ ·æœ¬")
    print(f"æ‰¹é‡å¤§å°: 64, æ··åˆç²¾åº¦è®­ç»ƒ, ä¼˜åŒ–æ•°æ®åŠ è½½")

    for epoch in range(num_epochs):
        # === è®­ç»ƒé˜¶æ®µ ===
        model.train()
        total_loss = 0
        num_batches = 0

        # æ·»åŠ æ›´è¯¦ç»†çš„è¿›åº¦ä¿¡æ¯
        progress_bar = tqdm(train_loader, desc=f"[Train Epoch {epoch}]", ncols=100)

        for batch in progress_bar:
            # ä½¿ç”¨éé˜»å¡ä¼ è¾“ï¼Œä½†ä¿æŒæ•°æ®ä¸ºfloat16
            x_original = batch['pixel_values'].cuda(non_blocking=True)
            x_augmented = batch['augmented_pixel_values'].cuda(non_blocking=True)

            optimizer.zero_grad()

            # æ··åˆç²¾åº¦è®­ç»ƒ
            with torch.cuda.amp.autocast():
                features_original = model(pixel_values=x_original).last_hidden_state[:, 0, :]
                features_augmented = model(pixel_values=x_augmented).last_hidden_state[:, 0, :]
                all_features = torch.cat([features_original, features_augmented], dim=0)
                loss = contrastive_loss_fn(all_features)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()
            num_batches += 1

            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰æŸå¤±
            if num_batches % 50 == 0:
                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        avg_train_loss = total_loss / num_batches
        train_losses.append(avg_train_loss)

        # === éªŒè¯é˜¶æ®µ ===
        model.eval()
        val_loss = 0
        val_batches = 0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"[Val Epoch {epoch}]", ncols=100, leave=False):
                x = batch['pixel_values'].cuda(non_blocking=True)
                with torch.cuda.amp.autocast():
                    output = model(pixel_values=x).last_hidden_state[:, 0, :]
                    batch_loss = contrastive_loss_fn(output)
                val_loss += batch_loss.item()
                val_batches += 1

        avg_val_loss = val_loss / val_batches
        val_losses.append(avg_val_loss)

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(avg_val_loss)

        print(f"[Epoch {epoch}] Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")

        # === æ¨¡å‹é€‰æ‹©å’Œä¿å­˜ ===
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), 'outputs/vit_lora_best.pt')
            print(f"â˜… æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼ŒéªŒè¯æŸå¤±: {best_val_loss:.4f}")
            patience_counter = 0
            model.save_pretrained('outputs/vit_lora_best_peft')
        else:
            patience_counter += 1

        # æ¯3è½®ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæ›´é¢‘ç¹ä¿å­˜ï¼‰
        if (epoch + 1) % 3 == 0:
            torch.save(model.state_dict(), f'outputs/vit_lora_epoch_{epoch + 1}.pt')
            model.save_pretrained(f'outputs/vit_lora_peft_epoch_{epoch + 1}')

        if patience_counter >= patience:
            print(f"âš ï¸ æ—©åœæœºåˆ¶è§¦å‘ï¼šè¿ç»­ {patience} ä¸ªepochéªŒè¯æŸå¤±æ— æ”¹å–„")
            print(f"è®­ç»ƒåœ¨ç¬¬ {epoch} è½®åœæ­¢")
            break

        if epoch > 0:
            train_improvement = train_losses[-2] - train_losses[-1]
            val_improvement = val_losses[-2] - val_losses[-1]
            print(f"  â†’ è®­ç»ƒæŸå¤±æ”¹å–„: {train_improvement:.4f}, éªŒè¯æŸå¤±æ”¹å–„: {val_improvement:.4f}")

    # === ä¿å­˜æœ€ç»ˆæ¨¡å‹ ===
    torch.save(model.state_dict(), 'outputs/vit_lora_final.pt')
    model.save_pretrained('outputs/vit_lora_final_peft')
    print("ğŸ è®­ç»ƒå®Œæˆ")
    print(f"ğŸ“ˆ æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ outputs/ ç›®å½•")

    # === è®­ç»ƒæŠ¥å‘Š ===
    print("\n=== è®­ç»ƒæ€»ç»“ ===")
    print(f"æ€»è®­ç»ƒè½®æ•°: {len(train_losses)}")
    print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.4f}")
    print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.4f}")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    print(f"æŸå¤±æ”¹å–„: {train_losses[0] - train_losses[-1]:.4f}")

    return model, processor


if __name__ == "__main__":
    train_vit_lora_optimized()
